\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{elsarticle-num}
\urlauthor{www.uni-due.de/\hashchar hm0124}{D. Belomestny\corref {mycorrespondingauthor}}
\emailauthor{denis.belomestny@uni-due.de}{D. Belomestny\corref {mycorrespondingauthor}}
\citation{christian1999monte}
\citation{rubinstein2016simulation}
\citation{glasserman2013monte}
\citation{dellaportas2012control}
\citation{mira2013zero}
\citation{brosse2018diffusion}
\citation{dimov2008monte}
\citation{mira2013zero}
\citation{oates2017control}
\providecommand \oddpage@label [2]{}
\Newlabel{mytitlenote}{1}
\Newlabel{mycorrespondingauthor}{1}
\Newlabel{address1}{a}
\Newlabel{address2}{b}
\Newlabel{address3}{c}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{belomestny2018stratified}
\citation{henderson1997variance}
\citation{duncan2016variance}
\citation{mira2013zero}
\citation{henderson1997variance}
\citation{dellaportas2012control}
\citation{brosse2018diffusion}
\citation{dellaportas2012control}
\citation{brosse2018diffusion}
\citation{dalalyan2017theoretical,durmus:moulines:2017}
\citation{moulines2018}
\@writefile{toc}{\contentsline {section}{\numberline {2}Setup}{3}{section.2}}
\newlabel{sec:setup}{{2}{3}{Setup}{section.2}{}}
\newlabel{eq:chain_gen}{{1}{3}{Setup}{equation.2.1}{}}
\newlabel{exam:langevin-algorithm}{{1}{3}{Setup}{example.1}{}}
\newlabel{eq:chain}{{2}{3}{Setup}{equation.2.2}{}}
\citation{dalalyan2017theoretical}
\citation{durmus:moulines:2017}
\citation{metropolis1953equation}
\citation{mengersen:tweedie:1996}
\newlabel{eq:stationary_distr}{{3}{4}{Setup}{equation.2.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Martingale representation and variance reduction}{5}{section.3}}
\newlabel{seq:mart_repr}{{3}{5}{Martingale representation and variance reduction}{section.3}{}}
\newlabel{prop:29032018a1}{{1}{5}{Martingale representation and variance reduction}{thm.1}{}}
\newlabel{eq:mart_repr}{{4}{5}{Martingale representation and variance reduction}{equation.3.4}{}}
\newlabel{eq:coeff_mart}{{5}{5}{Martingale representation and variance reduction}{equation.3.5}{}}
\newlabel{berm:sig_X}{{6}{6}{Martingale representation and variance reduction}{equation.3.6}{}}
\newlabel{eq:28082017a1}{{7}{6}{Martingale representation and variance reduction}{equation.3.7}{}}
\newlabel{eq:qpl}{{8}{6}{Martingale representation and variance reduction}{equation.3.8}{}}
\newlabel{eq:29032018a2}{{9}{7}{Martingale representation and variance reduction}{equation.3.9}{}}
\newlabel{eq:29032018a5}{{10}{7}{Martingale representation and variance reduction}{equation.3.10}{}}
\newlabel{eq:first-expression-bar-a-l-k}{{11}{7}{Martingale representation and variance reduction}{equation.3.11}{}}
\@writefile{tdo}{\contentsline {todo}{to be discussed should we really present two algorithms}{7}{section*.1}}
\pgfsyspdfmark {pgfid1}{20088094}{19990105}
\newlabel{eq:training-paths}{{12}{7}{Martingale representation and variance reduction}{equation.3.12}{}}
\newlabel{eq:definition-EstFunc}{{13}{7}{Martingale representation and variance reduction}{equation.3.13}{}}
\newlabel{eq:linear-regression-problem}{{14}{8}{Martingale representation and variance reduction}{equation.3.14}{}}
\newlabel{eq:a_est_int}{{15}{8}{Martingale representation and variance reduction}{equation.3.15}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Analysis of variance reduced ULA}{8}{section.4}}
\newlabel{sec:ula_analysis}{{4}{8}{Analysis of variance reduced ULA}{section.4}{}}
\newlabel{eq:29032018a3}{{16}{8}{Analysis of variance reduced ULA}{equation.4.16}{}}
\newlabel{eq:definition-G-p}{{17}{8}{Analysis of variance reduced ULA}{equation.4.17}{}}
\citation{durmus:moulines:2017}
\newlabel{eq:29032018a5}{{18}{9}{Analysis of variance reduced ULA}{equation.4.18}{}}
\newlabel{eq:29032018a4}{{19}{9}{Analysis of variance reduced ULA}{equation.4.19}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Squared conditional bias}{9}{subsection.4.1}}
\newlabel{eq:06042018a2}{{20}{9}{Squared conditional bias}{equation.4.20}{}}
\citation{durmus:moulines:2017}
\citation{durmus:moulines:2017}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Conditional variance}{10}{subsection.4.2}}
\newlabel{eq:29032018a6}{{21}{10}{Conditional variance}{equation.4.21}{}}
\newlabel{th:mr}{{3}{10}{Conditional variance}{thm.3}{}}
\newlabel{eq:17042018a1}{{22}{10}{Conditional variance}{equation.4.22}{}}
\citation{audibert2011robust}
\newlabel{eq:var-bound}{{23}{11}{Conditional variance}{equation.4.23}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Complexity analysis for ULA}{11}{section.5}}
\newlabel{sec:coeff}{{5}{11}{Complexity analysis for ULA}{section.5}{}}
\newlabel{eq:definition-train-alg}{{24}{11}{Complexity analysis for ULA}{equation.5.24}{}}
\newlabel{berm:theorem:regression_cv}{{4}{11}{Complexity analysis for ULA}{thm.4}{}}
\@writefile{tdo}{\contentsline {todo}{I would put $\mathcal  {G}_N$ there, no $X_j$}{11}{section*.2}}
\pgfsyspdfmark {pgfid2}{27237219}{25490665}
\pgfsyspdfmark {pgfid5}{34243870}{25502953}
\pgfsyspdfmark {pgfid6}{36357406}{25279039}
\@writefile{tdo}{\contentsline {todo}{I would put $\mathsf  {E}[g^2(X_l) | \mathcal  {G}_N]$ here}{11}{section*.3}}
\pgfsyspdfmark {pgfid7}{17600580}{19471725}
\pgfsyspdfmark {pgfid10}{34243870}{19484013}
\pgfsyspdfmark {pgfid11}{36357406}{19260099}
\newlabel{eq:regr_error}{{25}{11}{Complexity analysis for ULA}{equation.5.25}{}}
\citation{aida1994moment}
\newlabel{cor:dif_m}{{5}{12}{Complexity analysis for ULA}{thm.5}{}}
\newlabel{eq:main_bound}{{26}{12}{Complexity analysis for ULA}{equation.5.26}{}}
\newlabel{eq:ula_red_var}{{27}{12}{Complexity analysis for ULA}{equation.5.27}{}}
\@writefile{tdo}{\contentsline {todo}{to be checked: formulation of Lemma 6 in the vector case}{13}{section*.4}}
\pgfsyspdfmark {pgfid12}{20088094}{41007696}
\newlabel{a_decay}{{6}{13}{Complexity analysis for ULA}{thm.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Numerical results}{14}{section.6}}
\newlabel{sec:num}{{6}{14}{Numerical results}{section.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Polynomial approximation}{14}{subsection.6.1}}
\@writefile{tdo}{\contentsline {todo}{simplify this expression}{14}{section*.5}}
\pgfsyspdfmark {pgfid13}{15964082}{29291070}
\pgfsyspdfmark {pgfid16}{34243870}{29303358}
\pgfsyspdfmark {pgfid17}{36357406}{29079444}
\@writefile{tdo}{\contentsline {todo}{simplify the expression}{14}{section*.6}}
\pgfsyspdfmark {pgfid18}{20359591}{24972484}
\pgfsyspdfmark {pgfid21}{34243870}{24984772}
\pgfsyspdfmark {pgfid22}{36357406}{24760858}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Gaussian mixtures}{14}{subsection.6.2}}
\@writefile{tdo}{\contentsline {todo}{inconsistent notations withe the norm}{14}{section*.8}}
\pgfsyspdfmark {pgfid23}{9709742}{9236816}
\pgfsyspdfmark {pgfid26}{34243870}{9249104}
\pgfsyspdfmark {pgfid27}{36357406}{9025190}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:subfig1}{{1a}{15}{Subfigure 1a}{subfigure.1.1}{}}
\newlabel{sub@fig:subfig1}{{(a)}{a}{Subfigure 1a\relax }{subfigure.1.1}{}}
\newlabel{fig:subfig2}{{1b}{15}{Subfigure 1b}{subfigure.1.2}{}}
\newlabel{sub@fig:subfig2}{{(b)}{b}{Subfigure 1b\relax }{subfigure.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Histograms for Gaussian mixture. (a) 1-dimensional GM model: histograms of estimators for target function $f(x) = \mathrm  {e}^x$ on test sample (200 independent trajectories obtained by ULA algorithm). \textit  {Red} bins correspondent to ordinary weighted estimators $\pi ^N_n(f)$, \textit  {green} - variance-reduced estimators $\pi ^N_{1,n}(f)$.  (b) 2-dimensional GM model: histograms of estimators for target function $f(x) = x_1^2 + x_2^2 - \qopname  \relax o{cos}(x_1)$, \textit  {red} bins: $\pi ^N_n(f)$, \textit  {green}: $\pi ^N_{1,n}(f)$, \textit  {yellow}: $\pi ^N_{2,n}(f)$.\relax }}{15}{figure.caption.7}}
\newlabel{fig:1}{{1}{15}{Histograms for Gaussian mixture. (a) 1-dimensional GM model: histograms of estimators for target function $f(x) = \rme ^x$ on test sample (200 independent trajectories obtained by ULA algorithm). \textit {Red} bins correspondent to ordinary weighted estimators $\pi ^N_n(f)$, \textit {green} - variance-reduced estimators $\pi ^N_{1,n}(f)$.\\ (b) 2-dimensional GM model: histograms of estimators for target function $f(x) = x_1^2 + x_2^2 - \cos (x_1)$, \textit {red} bins: $\pi ^N_n(f)$, \textit {green}: $\pi ^N_{1,n}(f)$, \textit {yellow}: $\pi ^N_{2,n}(f)$.\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces Subfigure 1 list of figures text}}{15}{subfigure.1.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces Subfigure 2 list of figures text}}{15}{subfigure.1.2}}
\@writefile{tdo}{\contentsline {todo}{It is not clear which algorithm you are trying to illustrate; bowplots are better I guess}{15}{section*.9}}
\pgfsyspdfmark {pgfid28}{12756078}{25249864}
\pgfsyspdfmark {pgfid31}{34243870}{25262152}
\pgfsyspdfmark {pgfid32}{36357406}{25038238}
\@writefile{tdo}{\contentsline {todo}{far from being enough ! make it 100 times at least}{15}{section*.10}}
\pgfsyspdfmark {pgfid33}{11276834}{11789061}
\pgfsyspdfmark {pgfid36}{34243870}{11801349}
\pgfsyspdfmark {pgfid37}{36357406}{11577435}
\citation{dalalyan2017theoretical}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Gaussian Mixtures: Empirical variances of ordinary weighted and variance-reduced estimators on test sample.\relax }}{16}{table.caption.11}}
\newlabel{table:gm}{{1}{16}{Gaussian Mixtures: Empirical variances of ordinary weighted and variance-reduced estimators on test sample.\relax }{table.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces 1-dimensional GM model. Vertical axis is empirical variance on test sample, horizontal axis is the length of test trajectories obtained by ULA algorithm.Red traceplot correspondents to ordinary weighted estimator, green traseplot illustrates empirical variances of variance-reduced estimator for K=1.\relax }}{16}{figure.caption.12}}
\newlabel{fig:comparison}{{2}{16}{1-dimensional GM model. Vertical axis is empirical variance on test sample, horizontal axis is the length of test trajectories obtained by ULA algorithm.Red traceplot correspondents to ordinary weighted estimator, green traseplot illustrates empirical variances of variance-reduced estimator for K=1.\relax }{figure.caption.12}{}}
\@writefile{tdo}{\contentsline {todo}{what do you mean by that ?}{16}{section*.13}}
\pgfsyspdfmark {pgfid38}{10813254}{15584696}
\pgfsyspdfmark {pgfid41}{34243870}{15596984}
\pgfsyspdfmark {pgfid42}{36357406}{15373070}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Binary Logistic regression}{16}{subsection.6.3}}
\citation{dalalyan2017theoretical}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces  Binary Logistic Regression: Histograms of estimators for target function $f(\theta ) = 2 \theta _1^2 + 7 \theta _2^2$ on test sample. \textit  {Red} bins correspondent to ordinary weighted estimators $\pi ^N_n(f)$, \textit  {green} - variance-reduced estimators $\pi ^N_{1,n}(f)$and \textit  {yellow} - $\pi ^N_{2,n}(f)$.\relax }}{17}{figure.caption.14}}
\newlabel{fig:blr}{{3}{17}{Binary Logistic Regression: Histograms of estimators for target function $f(\theta ) = 2 \theta _1^2 + 7 \theta _2^2$ on test sample. \textit {Red} bins correspondent to ordinary weighted estimators $\pi ^N_n(f)$, \textit {green} - variance-reduced estimators $\pi ^N_{1,n}(f)$and \textit {yellow} - $\pi ^N_{2,n}(f)$.\relax }{figure.caption.14}{}}
\@writefile{tdo}{\contentsline {todo}{I think this is called a Zellner prior}{17}{section*.15}}
\pgfsyspdfmark {pgfid43}{18880689}{24518726}
\pgfsyspdfmark {pgfid46}{34243870}{24531014}
\pgfsyspdfmark {pgfid47}{36357406}{24307100}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces BLR: Empirical variance of ordinary weighted and variance-reduced estimators on test sample.\relax }}{18}{table.caption.16}}
\newlabel{table:blr}{{2}{18}{BLR: Empirical variance of ordinary weighted and variance-reduced estimators on test sample.\relax }{table.caption.16}{}}
\@writefile{tdo}{\contentsline {todo}{$\lambda $ is not specified}{18}{section*.17}}
\pgfsyspdfmark {pgfid48}{9604155}{35520176}
\pgfsyspdfmark {pgfid51}{34243870}{35532464}
\pgfsyspdfmark {pgfid52}{36357406}{35308550}
\@writefile{toc}{\contentsline {section}{\numberline {7}Proofs}{18}{section.7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Proof of Theorem\nobreakspace  {}\ref  {th:mr}}{18}{subsection.7.1}}
\newlabel{eq:definition-G-p-l}{{29}{18}{Proof of Theorem~\ref {th:mr}}{equation.7.29}{}}
\newlabel{eq:definition-differential-f-p}{{30}{18}{Proof of Theorem~\ref {th:mr}}{equation.7.30}{}}
\newlabel{eq:a_repr}{{7}{19}{Proof of Theorem~\ref {th:mr}}{thm.7}{}}
\newlabel{eq:sum_abar}{{31}{20}{Proof of Theorem~\ref {th:mr}}{equation.7.31}{}}
\newlabel{lem:06062018a1}{{8}{21}{Proof of Theorem~\ref {th:mr}}{thm.8}{}}
\newlabel{eq:06062018a1}{{32}{21}{Proof of Theorem~\ref {th:mr}}{equation.7.32}{}}
\newlabel{eq:06062018a2}{{33}{21}{Proof of Theorem~\ref {th:mr}}{equation.7.33}{}}
\newlabel{eq:06062018a3}{{34}{21}{Proof of Theorem~\ref {th:mr}}{equation.7.34}{}}
\newlabel{eq:definition-alpha}{{35}{22}{Proof of Theorem~\ref {th:mr}}{equation.7.35}{}}
\newlabel{rem:10062018a1}{{2}{22}{Proof of Theorem~\ref {th:mr}}{remark.2}{}}
\newlabel{eq:10062018a1}{{36}{22}{Proof of Theorem~\ref {th:mr}}{equation.7.36}{}}
\newlabel{lem:06062018a2}{{9}{22}{Proof of Theorem~\ref {th:mr}}{thm.9}{}}
\newlabel{eq:08062018b2}{{37}{22}{Proof of Theorem~\ref {th:mr}}{equation.7.37}{}}
\newlabel{lem:06062018a3}{{10}{22}{Proof of Theorem~\ref {th:mr}}{thm.10}{}}
\newlabel{eq:08062018b3}{{38}{22}{Proof of Theorem~\ref {th:mr}}{equation.7.38}{}}
\newlabel{eq:induc-derive}{{39}{23}{Proof of Theorem~\ref {th:mr}}{equation.7.39}{}}
\newlabel{eq:08062018a1}{{40}{23}{Proof of Theorem~\ref {th:mr}}{equation.7.40}{}}
\newlabel{eq:09062018a1}{{41}{23}{Proof of Theorem~\ref {th:mr}}{equation.7.41}{}}
\newlabel{eq:09062018a2}{{42}{23}{Proof of Theorem~\ref {th:mr}}{equation.7.42}{}}
\newlabel{lem:var_poincare}{{11}{24}{Proof of Theorem~\ref {th:mr}}{thm.11}{}}
\newlabel{eq:10062018a2}{{43}{25}{Proof of Theorem~\ref {th:mr}}{equation.7.43}{}}
\bibstyle{abbrv}
\bibdata{refs-1}
\bibcite{christian1999monte}{{1}{}{{}}{{}}}
\bibcite{rubinstein2016simulation}{{2}{}{{}}{{}}}
\bibcite{glasserman2013monte}{{3}{}{{}}{{}}}
\bibcite{dellaportas2012control}{{4}{}{{}}{{}}}
\bibcite{mira2013zero}{{5}{}{{}}{{}}}
\bibcite{brosse2018diffusion}{{6}{}{{}}{{}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Proof of Lemma\nobreakspace  {}\ref  {a_decay}}{26}{subsection.7.2}}
\bibcite{dimov2008monte}{{7}{}{{}}{{}}}
\bibcite{oates2017control}{{8}{}{{}}{{}}}
\bibcite{belomestny2018stratified}{{9}{}{{}}{{}}}
\bibcite{henderson1997variance}{{10}{}{{}}{{}}}
\bibcite{duncan2016variance}{{11}{}{{}}{{}}}
\bibcite{dalalyan2017theoretical}{{12}{}{{}}{{}}}
\bibcite{durmus:moulines:2017}{{13}{}{{}}{{}}}
\bibcite{moulines2018}{{14}{}{{}}{{}}}
\bibcite{metropolis1953equation}{{15}{}{{}}{{}}}
\bibcite{mengersen:tweedie:1996}{{16}{}{{}}{{}}}
\bibcite{audibert2011robust}{{17}{}{{}}{{}}}
\bibcite{aida1994moment}{{18}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
