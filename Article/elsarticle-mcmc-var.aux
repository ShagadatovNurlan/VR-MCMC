\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{elsarticle-num}
\urlauthor{www.uni-due.de/\hashchar hm0124}{D. Belomestny\corref {mycorrespondingauthor}}
\emailauthor{denis.belomestny@uni-due.de}{D. Belomestny\corref {mycorrespondingauthor}}
\citation{christian1999monte}
\citation{rubinstein2016simulation}
\citation{glasserman2013monte}
\citation{dellaportas2012control}
\citation{mira2013zero}
\citation{brosse2018diffusion}
\citation{dimov2008monte}
\citation{mira2013zero}
\citation{oates2017control}
\providecommand \oddpage@label [2]{}
\Newlabel{mytitlenote}{1}
\Newlabel{mycorrespondingauthor}{1}
\Newlabel{address1}{a}
\Newlabel{address2}{b}
\Newlabel{address3}{c}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{belomestny2018stratified}
\citation{henderson1997variance}
\citation{duncan2016variance}
\citation{mira2013zero}
\citation{henderson1997variance}
\citation{dellaportas2012control}
\citation{brosse2018diffusion}
\citation{dellaportas2012control}
\citation{brosse2018diffusion}
\citation{dalalyan2017theoretical,durmus:moulines:2017}
\citation{moulines2018}
\@writefile{toc}{\contentsline {section}{\numberline {2}Setup}{3}{section.2}}
\newlabel{sec:setup}{{2}{3}{Setup}{section.2}{}}
\newlabel{eq:chain_gen}{{1}{3}{Setup}{equation.2.1}{}}
\newlabel{exam:langevin-algorithm}{{1}{3}{Setup}{example.1}{}}
\newlabel{eq:chain}{{2}{3}{Setup}{equation.2.2}{}}
\citation{dalalyan2017theoretical}
\citation{durmus:moulines:2017}
\citation{metropolis1953equation}
\citation{mengersen:tweedie:1996}
\newlabel{eq:stationary_distr}{{3}{4}{Setup}{equation.2.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Martingale representation and variance reduction}{5}{section.3}}
\newlabel{seq:mart_repr}{{3}{5}{Martingale representation and variance reduction}{section.3}{}}
\newlabel{prop:29032018a1}{{1}{5}{Martingale representation and variance reduction}{thm.1}{}}
\newlabel{eq:mart_repr}{{4}{5}{Martingale representation and variance reduction}{equation.3.4}{}}
\newlabel{eq:coeff_mart}{{5}{5}{Martingale representation and variance reduction}{equation.3.5}{}}
\newlabel{berm:sig_X}{{6}{6}{Martingale representation and variance reduction}{equation.3.6}{}}
\newlabel{eq:28082017a1}{{7}{6}{Martingale representation and variance reduction}{equation.3.7}{}}
\newlabel{eq:qpl}{{8}{6}{Martingale representation and variance reduction}{equation.3.8}{}}
\newlabel{eq:29032018a2}{{9}{7}{Martingale representation and variance reduction}{equation.3.9}{}}
\newlabel{eq:29032018a5}{{10}{7}{Martingale representation and variance reduction}{equation.3.10}{}}
\newlabel{eq:first-expression-bar-a-l-k}{{11}{7}{Martingale representation and variance reduction}{equation.3.11}{}}
\newlabel{eq:est_a_direct}{{12}{7}{Martingale representation and variance reduction}{equation.3.12}{}}
\newlabel{eq:06042018a1}{{13}{8}{Martingale representation and variance reduction}{equation.3.13}{}}
\newlabel{eq:a_est_int}{{14}{8}{Martingale representation and variance reduction}{equation.3.14}{}}
\@writefile{tdo}{\contentsline {todo}{insert but later: For example, in the case of ULA algorithm we have \(\xi \sim \mathcal  {N}(0,I_d)\) and \(\Phi _l(x,\xi )=x-\gamma _{l}\mu (x)+\sqrt  {\gamma _{l}}\xi .\) Therefore if we use polynomials to approximate \((\mathaccentV {bar}016Q_{l}),\) then \((\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle a$}\mathaccent "0362{a}_{l,k})\) can be even computed in closed form, for more details see Section\nobreakspace  {}\ref  {sec:num}. }{8}{section*.1}}
\pgfsyspdfmark {pgfid1}{20088094}{32033237}
\@writefile{toc}{\contentsline {section}{\numberline {4}Analysis of variance reduced ULA}{8}{section.4}}
\newlabel{sec:ula_analysis}{{4}{8}{Analysis of variance reduced ULA}{section.4}{}}
\newlabel{eq:29032018a3}{{15}{8}{Analysis of variance reduced ULA}{equation.4.15}{}}
\citation{durmus:moulines:2017}
\newlabel{eq:29032018a5}{{16}{9}{Analysis of variance reduced ULA}{equation.4.16}{}}
\newlabel{eq:29032018a4}{{17}{9}{Analysis of variance reduced ULA}{equation.4.17}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Squared conditional bias}{9}{subsection.4.1}}
\newlabel{eq:06042018a2}{{18}{9}{Squared conditional bias}{equation.4.18}{}}
\citation{durmus:moulines:2017}
\citation{durmus:moulines:2017}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Conditional variance}{10}{subsection.4.2}}
\newlabel{eq:29032018a6}{{19}{10}{Conditional variance}{equation.4.19}{}}
\newlabel{th:mr}{{3}{10}{Conditional variance}{thm.3}{}}
\newlabel{eq:17042018a1}{{20}{10}{Conditional variance}{equation.4.20}{}}
\newlabel{eq:var-bound}{{21}{11}{Conditional variance}{equation.4.21}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Complexity analysis for ULA}{11}{section.5}}
\newlabel{sec:coeff}{{5}{11}{Complexity analysis for ULA}{section.5}{}}
\newlabel{eq:est_a_direct}{{22}{11}{Complexity analysis for ULA}{equation.5.22}{}}
\newlabel{eq:06042018a1}{{23}{11}{Complexity analysis for ULA}{equation.5.23}{}}
\newlabel{eq:a_est_int}{{24}{11}{Complexity analysis for ULA}{equation.5.24}{}}
\citation{audibert2011robust}
\@writefile{tdo}{\contentsline {todo}{insert but later: For example, in the case of ULA algorithm we have \(\xi \sim \mathcal  {N}(0,I_d)\) and \(\Phi _l(x,\xi )=x-\gamma _{l}\mu (x)+\sqrt  {\gamma _{l}}\xi .\) Therefore if we use polynomials to approximate \((\mathaccentV {bar}016Q_{l}),\) then \((\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle a$}\mathaccent "0362{a}_{l,k})\) can be even computed in closed form, for more details see Section\nobreakspace  {}\ref  {sec:num}. }{12}{section*.2}}
\pgfsyspdfmark {pgfid2}{20088094}{41858935}
\newlabel{berm:theorem:regression_cv}{{4}{12}{Complexity analysis for ULA}{thm.4}{}}
\@writefile{tdo}{\contentsline {todo}{I would put $\mathcal  {G}_N$ there, no $X_j$}{12}{section*.3}}
\pgfsyspdfmark {pgfid3}{27237219}{31176117}
\pgfsyspdfmark {pgfid6}{34243870}{31188405}
\pgfsyspdfmark {pgfid7}{36357406}{30964491}
\@writefile{tdo}{\contentsline {todo}{I would put $\mathsf  {E}[g^2(X_l) | \mathcal  {G}_N]$ here}{12}{section*.4}}
\pgfsyspdfmark {pgfid8}{17600580}{24789970}
\pgfsyspdfmark {pgfid11}{34243870}{24802258}
\pgfsyspdfmark {pgfid12}{36357406}{24578344}
\newlabel{eq:regr_error}{{25}{12}{Complexity analysis for ULA}{equation.5.25}{}}
\newlabel{cor:dif_m}{{5}{12}{Complexity analysis for ULA}{thm.5}{}}
\citation{aida1994moment}
\newlabel{eq:main_bound}{{26}{13}{Complexity analysis for ULA}{equation.5.26}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Complexity analysis of variance reduction for the ULA algorithm}{13}{subsection.5.1}}
\newlabel{eq:ula_red_var}{{27}{13}{Complexity analysis of variance reduction for the ULA algorithm}{equation.5.27}{}}
\newlabel{a_decay}{{6}{14}{Complexity analysis of variance reduction for the ULA algorithm}{thm.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Numerical results}{15}{section.6}}
\newlabel{sec:num}{{6}{15}{Numerical results}{section.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Polynomial approximation}{15}{subsection.6.1}}
\@writefile{tdo}{\contentsline {todo}{simplify this expression}{15}{section*.5}}
\pgfsyspdfmark {pgfid13}{15964082}{31150107}
\pgfsyspdfmark {pgfid16}{34243870}{31162395}
\pgfsyspdfmark {pgfid17}{36357406}{30938481}
\@writefile{tdo}{\contentsline {todo}{simplify the expression}{15}{section*.6}}
\pgfsyspdfmark {pgfid18}{20359591}{26993045}
\pgfsyspdfmark {pgfid21}{34243870}{27005333}
\pgfsyspdfmark {pgfid22}{36357406}{26781419}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Gaussian mixtures}{15}{subsection.6.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:subfig1}{{1a}{16}{Subfigure 1a}{subfigure.1.1}{}}
\newlabel{sub@fig:subfig1}{{(a)}{a}{Subfigure 1a\relax }{subfigure.1.1}{}}
\newlabel{fig:subfig2}{{1b}{16}{Subfigure 1b}{subfigure.1.2}{}}
\newlabel{sub@fig:subfig2}{{(b)}{b}{Subfigure 1b\relax }{subfigure.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Histograms for Gaussian mixture. (a) 1-dimensional GM model: histograms of estimators for target function $f(x) = e^x$ on test sample (200 independent trajectories obtained by ULA algorithm). \textit  {Red} bins correspondent to ordinary weighted estimators $\pi ^N_n(f)$, \textit  {green} - variance-reduced estimators $\pi ^N_{1,n}(f)$.  (b) 2-dimensional GM model: histograms of estimators for target function $f(x) = x_1^2 + x_2^2 - \qopname  \relax o{cos}(x_1)$, \textit  {red} bins: $\pi ^N_n(f)$, \textit  {green}: $\pi ^N_{1,n}(f)$, \textit  {yellow}: $\pi ^N_{2,n}(f)$.\relax }}{16}{figure.caption.7}}
\newlabel{fig:1}{{1}{16}{Histograms for Gaussian mixture. (a) 1-dimensional GM model: histograms of estimators for target function $f(x) = e^x$ on test sample (200 independent trajectories obtained by ULA algorithm). \textit {Red} bins correspondent to ordinary weighted estimators $\pi ^N_n(f)$, \textit {green} - variance-reduced estimators $\pi ^N_{1,n}(f)$.\\ (b) 2-dimensional GM model: histograms of estimators for target function $f(x) = x_1^2 + x_2^2 - \cos (x_1)$, \textit {red} bins: $\pi ^N_n(f)$, \textit {green}: $\pi ^N_{1,n}(f)$, \textit {yellow}: $\pi ^N_{2,n}(f)$.\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces Subfigure 1 list of figures text}}{16}{subfigure.1.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces Subfigure 2 list of figures text}}{16}{subfigure.1.2}}
\citation{dalalyan2017theoretical}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Gaussian Mixtures: Empirical variances of ordinary weighted and variance-reduced estimators on test sample.\relax }}{17}{table.caption.8}}
\newlabel{table:gm}{{1}{17}{Gaussian Mixtures: Empirical variances of ordinary weighted and variance-reduced estimators on test sample.\relax }{table.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces 1-dimensional GM model. Vertical axis is empirical variance on test sample, horizontal axis is the length of test trajectories obtained by ULA algorithm.Red traceplot correspondents to ordinary weighted estimator, green traseplot illustrates empirical variances of variance-reduced estimator for K=1.\relax }}{17}{figure.caption.9}}
\newlabel{fig:comparison}{{2}{17}{1-dimensional GM model. Vertical axis is empirical variance on test sample, horizontal axis is the length of test trajectories obtained by ULA algorithm.Red traceplot correspondents to ordinary weighted estimator, green traseplot illustrates empirical variances of variance-reduced estimator for K=1.\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Binary Logistic regression}{17}{subsection.6.3}}
\citation{dalalyan2017theoretical}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces  Binary Logistic Regression: Histograms of estimators for target function $f(\theta ) = 2 \theta _1^2 + 7 \theta _2^2$ on test sample. \textit  {Red} bins correspondent to ordinary weighted estimators $\pi ^N_n(f)$, \textit  {green} - variance-reduced estimators $\pi ^N_{1,n}(f)$and \textit  {yellow} - $\pi ^N_{2,n}(f)$.\relax }}{18}{figure.caption.10}}
\newlabel{fig:blr}{{3}{18}{Binary Logistic Regression: Histograms of estimators for target function $f(\theta ) = 2 \theta _1^2 + 7 \theta _2^2$ on test sample. \textit {Red} bins correspondent to ordinary weighted estimators $\pi ^N_n(f)$, \textit {green} - variance-reduced estimators $\pi ^N_{1,n}(f)$and \textit {yellow} - $\pi ^N_{2,n}(f)$.\relax }{figure.caption.10}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces BLR: Empirical variance of ordinary weighted and variance-reduced estimators on test sample.\relax }}{18}{table.caption.11}}
\newlabel{table:blr}{{2}{18}{BLR: Empirical variance of ordinary weighted and variance-reduced estimators on test sample.\relax }{table.caption.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Proofs}{19}{section.7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Proof of Theorem\nobreakspace  {}\ref  {th:mr}}{19}{subsection.7.1}}
\newlabel{eq:definition-G-p-l}{{28}{19}{Proof of Theorem~\ref {th:mr}}{equation.7.28}{}}
\newlabel{eq:definition-differential-f-p}{{29}{19}{Proof of Theorem~\ref {th:mr}}{equation.7.29}{}}
\newlabel{eq:a_repr}{{7}{20}{Proof of Theorem~\ref {th:mr}}{thm.7}{}}
\newlabel{eq:sum_abar}{{30}{21}{Proof of Theorem~\ref {th:mr}}{equation.7.30}{}}
\newlabel{lem:06062018a1}{{8}{22}{Proof of Theorem~\ref {th:mr}}{thm.8}{}}
\newlabel{eq:06062018a1}{{31}{22}{Proof of Theorem~\ref {th:mr}}{equation.7.31}{}}
\newlabel{eq:06062018a2}{{32}{22}{Proof of Theorem~\ref {th:mr}}{equation.7.32}{}}
\newlabel{eq:06062018a3}{{33}{22}{Proof of Theorem~\ref {th:mr}}{equation.7.33}{}}
\newlabel{eq:definition-alpha}{{34}{23}{Proof of Theorem~\ref {th:mr}}{equation.7.34}{}}
\newlabel{rem:10062018a1}{{2}{23}{Proof of Theorem~\ref {th:mr}}{remark.2}{}}
\newlabel{eq:10062018a1}{{35}{23}{Proof of Theorem~\ref {th:mr}}{equation.7.35}{}}
\newlabel{lem:06062018a2}{{9}{23}{Proof of Theorem~\ref {th:mr}}{thm.9}{}}
\newlabel{eq:08062018b2}{{36}{23}{Proof of Theorem~\ref {th:mr}}{equation.7.36}{}}
\newlabel{lem:06062018a3}{{10}{23}{Proof of Theorem~\ref {th:mr}}{thm.10}{}}
\newlabel{eq:08062018b3}{{37}{23}{Proof of Theorem~\ref {th:mr}}{equation.7.37}{}}
\newlabel{eq:induc-derive}{{38}{24}{Proof of Theorem~\ref {th:mr}}{equation.7.38}{}}
\newlabel{eq:08062018a1}{{39}{24}{Proof of Theorem~\ref {th:mr}}{equation.7.39}{}}
\newlabel{eq:09062018a1}{{40}{24}{Proof of Theorem~\ref {th:mr}}{equation.7.40}{}}
\newlabel{eq:09062018a2}{{41}{24}{Proof of Theorem~\ref {th:mr}}{equation.7.41}{}}
\newlabel{lem:var_poincare}{{11}{25}{Proof of Theorem~\ref {th:mr}}{thm.11}{}}
\newlabel{eq:10062018a2}{{42}{26}{Proof of Theorem~\ref {th:mr}}{equation.7.42}{}}
\bibstyle{abbrv}
\bibdata{refs-1}
\bibcite{christian1999monte}{{1}{}{{}}{{}}}
\bibcite{rubinstein2016simulation}{{2}{}{{}}{{}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Proof of Lemma\nobreakspace  {}\ref  {a_decay}}{27}{subsection.7.2}}
\bibcite{glasserman2013monte}{{3}{}{{}}{{}}}
\bibcite{dellaportas2012control}{{4}{}{{}}{{}}}
\bibcite{mira2013zero}{{5}{}{{}}{{}}}
\bibcite{brosse2018diffusion}{{6}{}{{}}{{}}}
\bibcite{dimov2008monte}{{7}{}{{}}{{}}}
\bibcite{oates2017control}{{8}{}{{}}{{}}}
\bibcite{belomestny2018stratified}{{9}{}{{}}{{}}}
\bibcite{henderson1997variance}{{10}{}{{}}{{}}}
\bibcite{duncan2016variance}{{11}{}{{}}{{}}}
\bibcite{dalalyan2017theoretical}{{12}{}{{}}{{}}}
\bibcite{durmus:moulines:2017}{{13}{}{{}}{{}}}
\bibcite{moulines2018}{{14}{}{{}}{{}}}
\bibcite{metropolis1953equation}{{15}{}{{}}{{}}}
\bibcite{mengersen:tweedie:1996}{{16}{}{{}}{{}}}
\bibcite{audibert2011robust}{{17}{}{{}}{{}}}
\bibcite{aida1994moment}{{18}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
